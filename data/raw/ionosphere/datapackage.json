{"name":"uci_ionosphere","title":"Ionosphere","description":"Classification of radar returns from the ionosphere\n\n# Source:  \n\nDonor:  \nVince Sigillito (vgs '@' aplcen.apl.jhu.edu)  \n\nSource: \nSpace Physics Group\nApplied Physics Laboratory\nJohns Hopkins University\nJohns Hopkins RoadLaurel, MD 20723 \n\n# Data Set Information:\nThis radar data was collected by a system in Goose Bay, Labrador. This system consists of a phased array of 16 high-frequency antennas with a total transmitted power on the order of 6.4 kilowatts. See the paper for more details. The targets were free electrons in the ionosphere. \"Good\" radar returns are those showing evidence of some type of structure in the ionosphere. \"Bad\" returns are those that do not; their signals pass through the ionosphere.  \n\nReceived signals were processed using an autocorrelation function whose arguments are the time of a pulse and the pulse number. There were 17 pulse numbers for the Goose Bay system. Instances in this databse are described by 2 attributes per pulse number, corresponding to the complex values returned by the function resulting from the complex electromagnetic signal.\n\n# Attribute Information:\n* All 34 are continuous\n* The 35th attribute is either \"good\" or \"bad\" according to the definition summarized above. This is a binary classification task.\n\n# Relevant Papers:\nSigillito, V. G., Wing, S. P., Hutton, L. V., \\& Baker, K. B. (1989). Classification of radar returns from the ionosphere using neural networks. Johns Hopkins APL Technical Digest, 10, 262-266. \n\n# Papers That Cite This Data Set1:\n* Jeroen Eggermont and Joost N. Kok and Walter A. Kosters. Genetic Programming for data classification: partitioning the search space. SAC. 2004.\n * Jennifer G. Dy and Carla Brodley. Feature Selection for Unsupervised Learning. Journal of Machine Learning Research, 5. 2004.\n * Mikhail Bilenko and Sugato Basu and Raymond J. Mooney. Integrating constraints and metric learning in semi-supervised clustering. ICML. 2004.\n * Zhi-Hua Zhou and Yuan Jiang. NeC4.5: Neural Ensemble Based C4.5. IEEE Trans. Knowl. Data Eng, 16. 2004.\n * Hyunsoo Kim and Se Hyun Park. Data Reduction in Support Vector Machines by a Kernelized Ionic Interaction Model. SDM. 2004.\n * Glenn Fung and M. Murat Dundar and Jinbo Bi and Bharat Rao. A fast iterative algorithm for fisher discriminant using heterogeneous kernels. ICML. 2004.\n * Predrag Radivojac and Zoran Obradovic and A. Keith Dunker and Slobodan Vucetic. Feature Selection Filters Based on the Permutation Test. ECML. 2004.\n * Dmitriy Fradkin and David Madigan. Experiments with random projections for machine learning. KDD. 2003.\n * Michael L. Raymer and Travis E. Doom and Leslie A. Kuhn and William F. Punch. Knowledge discovery in medical and biological datasets using a hybrid Bayes classifier/evolutionary algorithm. IEEE Transactions on Systems, Man, and Cybernetics, Part B, 33. 2003.\n * Marina Skurichina and Ludmila Kuncheva and Robert P W Duin. Bagging and Boosting for the Nearest Mean Classifier: Effects of Sample Size on Diversity and Accuracy. Multiple Classifier Systems. 2002.\n * Robert Burbidge and Matthew Trotter and Bernard F. Buxton and Sean B. Holden. STAR - Sparsity through Automated Rejection. IWANN (1). 2001.\n * Marina Skurichina and Robert P W Duin. Boosting in Linear Discriminant Analysis. Multiple Classifier Systems. 2000.\n * Lorne Mason and Peter L. Bartlett and Jonathan Baxter. Improved Generalization Through Explicit Optimization of Margins. Machine Learning, 38. 2000.\n * Justin Bradley and Kristin P. Bennett and Bennett A. Demiriz. Constrained K-Means Clustering. Microsoft Research Dept. of Mathematical Sciences One Microsoft Way Dept. of Decision Sciences and Eng. Sys. 2000.\n * Jennifer G. Dy and Carla Brodley. Feature Subset Selection and Order Identification for Unsupervised Learning. ICML. 2000.\n * P. S and Bradley K. P and Bennett A. Demiriz. Constrained K-Means Clustering. Microsoft Research Dept. of Mathematical Sciences One Microsoft Way Dept. of Decision Sciences and Eng. Sys. 2000.\n * Juan J. Rodr##guez and Carlos J. Alonso and Henrik Bostrom. Boosting Interval Based Literals. 2000.\n * Colin Campbell and Nello Cristianini and Alex J. Smola. Query Learning with Large Margin Classifiers. ICML. 2000.\n * Stavros J. Perantonis and Vassilis Virvilis. Input Feature Extraction for Multilayered Perceptrons Using Supervised Principal Component Analysis. Neural Processing Letters, 10. 1999.\n * David M J Tax and Robert P W Duin. Support vector domain description. Pattern Recognition Letters, 20. 1999.\n * Art B. Owen. Tubular neighbors for regression and classification. Stanford University. 1999.\n * Chun-Nan Hsu and Hilmar Schuschel and Ya-Ting Yang. The ANNIGMA-Wrapper Approach to Neural Nets Feature Selection for Knowledge Discovery and Data Mining. Institute of Information Science. 1999.\n * Lorne Mason and Jonathan Baxter and Peter L. Bartlett and Marcus Frean. Boosting Algorithms as Gradient Descent. NIPS. 1999.\n * Kai Ming Ting and Ian H. Witten. Issues in Stacked Generalization. J. Artif. Intell. Res. (JAIR, 10. 1999.\n * Stephen D. Bay. Nearest neighbor classification from multiple feature subsets. Intell. Data Anal, 3. 1999.\n * Robert E. Schapire and Yoav Freund and Peter Bartlett and Wee Sun Lee. The Annals of Statistics, to appear. Boosting the Margin: A New Explanation for the Effectiveness of Voting Methods. AT&T Labs. 1998.\n * Lorne Mason and Peter L. Bartlett and Jonathan Baxter. Direct Optimization of Margins Improves Generalization in Combined Classifiers. NIPS. 1998.\n * Richard Maclin. Boosting Classifiers Regionally. AAAI/IAAI. 1998.\n * Kristin P. Bennett and Erin J. Bredensteiner. A Parametric Optimization Method for Machine Learning. INFORMS Journal on Computing, 9. 1997.\n * Aynur Akkus and H. Altay GÃ¼venir. K Nearest Neighbor Classification on Feature Projections. ICML. 1996.\n * Federico Divina and Elena Marchiori. Handling Continuous Attributes in an Evolutionary Inductive Learner. Department of Computer Science Vrije Universiteit.\n * Glenn Fung and Sathyakama Sandilya and R. Bharat Rao. Rule extraction from Linear Support Vector Machines. Computer-Aided Diagnosis & Therapy, Siemens Medical Solutions, Inc.\n * Karthik Ramakrishnan. UNIVERSITY OF MINNESOTA.\n * Michalis K. Titsias and Aristidis Likas. Shared Kernel Models for Class Conditional Density Estimation.\n * Alexander K. Seewald. Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften.\n * Wl/odzisl/aw Duch and Karol Grudzinski and Geerd H. F Diercksen. Minimal distance neural methods. Department of Computer Methods, Nicholas Copernicus University.\n * Andrew Watkins and Jon Timmis and Lois C. Boggess. Artificial Immune Recognition System (AIRS): An ImmuneInspired Supervised Learning Algorithm. (abw5,jt6@kent.ac.uk) Computing Laboratory, University of Kent.\n * Aynur Akku and H. Altay Guvenir. Weighting Features in k Nearest Neighbor Classification on Feature Projections. Department of Computer Engineering and Information Science Bilkent University.\n * Krzysztof Grabczewski and Wl/odzisl/aw Duch. THE SEPARABILITY OF SPLIT VALUE CRITERION. Department of Computer Methods, Nicolaus Copernicus University.\n * Christos Emmanouilidis and A. Hunter and Dr J. MacIntyre. A Multiobjective Evolutionary Setting for Feature Selection and a Commonality-Based Crossover Operator. Centre for Adaptive Systems, School of Computing, Engineering and Technology University of Sunderland.\n * Chiranjib Bhattacharyya. Robust Classification of noisy data using Second Order Cone Programming approach. Dept. Computer Science and Automation, Indian Institute of Science.\n * Ayhan Demiriz and Kristin P. Bennett. Chapter 1 OPTIMIZATIONAPPROACHESTOSEMI-SUPERVISED LEARNING. Department of Decision Sciences and Engineering Systems & Department of Mathematical Sciences, Rensselaer Polytechnic Institute.\n * Isabelle Alvarez and Stephan Bernard. Ranking Cases with Decision Trees: a Geometric Method that Preserves Intelligibility.\n * Christos Dimitrakakis and Samy Bengioy. Online Policy Adaptation for Ensemble Classifiers. IDIAP.\n * Rajesh Parekh and Jihoon Yang and Vasant Honavar. Constructive Neural-Network Learning Algorithms for Pattern Classification.\n * Alain Rakotomamonjy. Leave-One-Out errors in Bipartite Ranking SVM. PSI CNRS FRE2645 INSA de Rouen Avenue de l'universite.\n * Wl/odzisl/aw Duch and Karol Grudzinski. Meta-learning: searching in the model space. Department of Computer Methods, Nicholas Copernicus University.\n * Federico Divina and Elena Marchiori. Knowledge-Based Evolutionary Search for Inductive Concept Learning. Vrije Universiteit of Amsterdam.\n * Charles Campbell and Nello Cristianini. Simple Learning Algorithms for Training Support Vector Machines. Dept. of Engineering Mathematics.\n * K. A. J Doherty and Rolf Adams and Neil Davey. Unsupervised Learning with Normalised Data and Non-Euclidean Norms. University of Hertfordshire.\n * Michael Lindenbaum and Shaul Markovitch and Dmitry Rusakov. Selective Sampling Using Random Field Modelling.\n * Christos Emmanouilidis and Anthony Hunter. A Comparison of Crossover Operators in Neural Network Feature Selection with Multiobjective Evolutionary Algorithms. Centre for Adaptive Systems, School of Computing, Engineering and Technology University of Sunderland.\n * Chiranjib Bhattacharyya and Pannagadatta K. S and Alexander J. Smola. A Second order Cone Programming Formulation for Classifying Missing Data. Department of Computer Science and Automation Indian Institute of Science.\n * Perry Moerland. Mixtures of latent variable models for density estimation and classification. E S E A R C H R E P R O R T I D I A P D a l l e M o l l e I n s t i t u t e f o r Pe r cep t ua l A r t i f i c i a l Intelligence .\n * Markus Breitenbach and Rodney Nielsen and Gregory Z. Grudic. Probabilistic Random Forests: Predicting Data Point Specific Misclassification Probabilities. Department of Computer Science University of Colorado.\n\n# Citation Request:\nPlease refer to the Machine Learning [Repository's citation policy.](http://archive.ics.uci.edu/ml/citation_policy.html)\n[1] Papers were automatically harvested and associated with this data set, in collaborationwith [Rexa.info](http://rexa.info/)\n\n**_Source:_** http://archive.ics.uci.edu/ml/datasets/Ionosphere","homepage":"https://data.world/uci/ionosphere","license":"Other","resources":[{"name":"ionosphere_data","schema":{"fields":[{"name":"column_a","title":"Column A","type":"boolean","rdfType":"http://www.w3.org/2001/XMLSchema#boolean","dwSourceId":"column_a"},{"name":"column_b","title":"Column B","type":"boolean","rdfType":"http://www.w3.org/2001/XMLSchema#boolean","dwSourceId":"column_b"},{"name":"column_c","title":"Column C","type":"number","rdfType":"http://www.w3.org/2001/XMLSchema#decimal","dwSourceId":"column_c"},{"name":"column_d","title":"Column D","type":"number","rdfType":"http://www.w3.org/2001/XMLSchema#decimal","dwSourceId":"column_d"},{"name":"column_e","title":"Column E","type":"number","rdfType":"http://www.w3.org/2001/XMLSchema#decimal","dwSourceId":"column_e"},{"name":"column_f","title":"Column F","type":"number","rdfType":"http://www.w3.org/2001/XMLSchema#decimal","dwSourceId":"column_f"},{"name":"column_g","title":"Column G","type":"number","rdfType":"http://www.w3.org/2001/XMLSchema#decimal","dwSourceId":"column_g"},{"name":"column_h","title":"Column H","type":"number","rdfType":"http://www.w3.org/2001/XMLSchema#decimal","dwSourceId":"column_h"},{"name":"column_i","title":"Column I","type":"number","rdfType":"http://www.w3.org/2001/XMLSchema#decimal","dwSourceId":"column_i"},{"name":"column_j","title":"Column J","type":"number","rdfType":"http://www.w3.org/2001/XMLSchema#decimal","dwSourceId":"column_j"},{"name":"column_k","title":"Column K","type":"number","rdfType":"http://www.w3.org/2001/XMLSchema#decimal","dwSourceId":"column_k"},{"name":"column_l","title":"Column L","type":"number","rdfType":"http://www.w3.org/2001/XMLSchema#decimal","dwSourceId":"column_l"},{"name":"column_m","title":"Column M","type":"number","rdfType":"http://www.w3.org/2001/XMLSchema#decimal","dwSourceId":"column_m"},{"name":"column_n","title":"Column N","type":"number","rdfType":"http://www.w3.org/2001/XMLSchema#decimal","dwSourceId":"column_n"},{"name":"column_o","title":"Column O","type":"number","rdfType":"http://www.w3.org/2001/XMLSchema#decimal","dwSourceId":"column_o"},{"name":"column_p","title":"Column P","type":"number","rdfType":"http://www.w3.org/2001/XMLSchema#decimal","dwSourceId":"column_p"},{"name":"column_q","title":"Column Q","type":"number","rdfType":"http://www.w3.org/2001/XMLSchema#decimal","dwSourceId":"column_q"},{"name":"column_r","title":"Column R","type":"number","rdfType":"http://www.w3.org/2001/XMLSchema#decimal","dwSourceId":"column_r"},{"name":"column_s","title":"Column S","type":"number","rdfType":"http://www.w3.org/2001/XMLSchema#decimal","dwSourceId":"column_s"},{"name":"column_t","title":"Column T","type":"number","rdfType":"http://www.w3.org/2001/XMLSchema#decimal","dwSourceId":"column_t"},{"name":"column_u","title":"Column U","type":"number","rdfType":"http://www.w3.org/2001/XMLSchema#decimal","dwSourceId":"column_u"},{"name":"column_v","title":"Column V","type":"number","rdfType":"http://www.w3.org/2001/XMLSchema#decimal","dwSourceId":"column_v"},{"name":"column_w","title":"Column W","type":"number","rdfType":"http://www.w3.org/2001/XMLSchema#decimal","dwSourceId":"column_w"},{"name":"column_x","title":"Column X","type":"number","rdfType":"http://www.w3.org/2001/XMLSchema#decimal","dwSourceId":"column_x"},{"name":"column_y","title":"Column Y","type":"number","rdfType":"http://www.w3.org/2001/XMLSchema#decimal","dwSourceId":"column_y"},{"name":"column_z","title":"Column Z","type":"number","rdfType":"http://www.w3.org/2001/XMLSchema#decimal","dwSourceId":"column_z"},{"name":"column_aa","title":"Column AA","type":"number","rdfType":"http://www.w3.org/2001/XMLSchema#decimal","dwSourceId":"column_aa"},{"name":"column_ab","title":"Column AB","type":"number","rdfType":"http://www.w3.org/2001/XMLSchema#decimal","dwSourceId":"column_ab"},{"name":"column_ac","title":"Column AC","type":"number","rdfType":"http://www.w3.org/2001/XMLSchema#decimal","dwSourceId":"column_ac"},{"name":"column_ad","title":"Column AD","type":"number","rdfType":"http://www.w3.org/2001/XMLSchema#decimal","dwSourceId":"column_ad"},{"name":"column_ae","title":"Column AE","type":"number","rdfType":"http://www.w3.org/2001/XMLSchema#decimal","dwSourceId":"column_ae"},{"name":"column_af","title":"Column AF","type":"number","rdfType":"http://www.w3.org/2001/XMLSchema#decimal","dwSourceId":"column_af"},{"name":"column_ag","title":"Column AG","type":"number","rdfType":"http://www.w3.org/2001/XMLSchema#decimal","dwSourceId":"column_ag"},{"name":"column_ah","title":"Column AH","type":"number","rdfType":"http://www.w3.org/2001/XMLSchema#decimal","dwSourceId":"column_ah"},{"name":"column_ai","title":"Column AI","type":"string","rdfType":"http://www.w3.org/2001/XMLSchema#string","dwSourceId":"column_ai"}]},"path":"data/ionosphere_data.csv","format":"csv"},{"name":"original/Index.txt","path":"original/Index.txt","format":"txt","mediatype":"text/plain","bytes":123},{"name":"original/ionosphere.data.csv","path":"original/ionosphere.data.csv","format":"csv","mediatype":"text/csv","bytes":76467},{"name":"original/ionosphere.names.txt","path":"original/ionosphere.names.txt","format":"txt","mediatype":"text/plain","bytes":3116}],"keywords":["uci","physical","integer","real","classification"]}