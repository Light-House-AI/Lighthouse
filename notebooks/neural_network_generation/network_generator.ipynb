{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import floor, ceil\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, classification_report, roc_auc_score\n",
    "from keras.utils import np_utils\n",
    "from ...server.automl.neural_network.neural_network import NeuralNetwork\n",
    "from ...server.automl.neural_network.fc_layer import FCLayer\n",
    "from ...server.automl.neural_network.activation_layer import ActivationLayer\n",
    "from ...server.automl.neural_network.activation_functions import sigmoid, sigmoid_derivative, identity, identity_derivative, tanh, tanh_derivative, relu, relu_derivative\n",
    "from ...server.automl.neural_network.loss_functions import mse, mse_derivative\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"type\": \"Regression\",\n",
    "    #\"number_of_layers\": [3, 4, 5],\n",
    "    #\"hidden_layer_sizes\": [2, 1, 1, 1, 1],\n",
    "    \"output_layer_size\": 1,\n",
    "    #\"activation_function\": \"sigmoid\",\n",
    "    #\"learning_rate\": 0.1,\n",
    "    #\"epochs\": 100,\n",
    "    #\"batch_size\": 1,\n",
    "    #\"loss_function\": \"mean_squared_error\",\n",
    "    #\"optimizer\": \"adam\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_wine(return_X_y=True)\n",
    "print(X.shape, Y.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.2)\n",
    "#Normalizing data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkGenerator:\n",
    "    def __init__(self, X_train, y_train, X_test, y_test, type):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.type = type\n",
    "        self.input_layer_size = X_train.shape[1]\n",
    "        if(self.type == \"Regression\"):\n",
    "            self.output_layer_size = 1\n",
    "            self.activation_function = relu\n",
    "            self.activation_function_derivative = relu_derivative\n",
    "        else: \n",
    "            self.output_layer_size = y_train.shape[1]\n",
    "            self.activation_function = tanh\n",
    "            self.activation_function_derivative = tanh_derivative\n",
    "        self.middle_layer_size = ceil((2*(self.input_layer_size + self.output_layer_size))/3)\n",
    "        \n",
    "#interpolating the hidden layer sizes\n",
    "    def __interpolate_hidden_layer_sizes(self, middle_layer_size, number_of_layers):\n",
    "        hidden_layer_sizes = [0 for x in range(number_of_layers)]\n",
    "        layers_before_middle = floor(number_of_layers/2) #2\n",
    "        layers_after_middle = ceil(number_of_layers/2) - 1 #2\n",
    "        hidden_layer_sizes[layers_before_middle] = middle_layer_size\n",
    "        index = layers_before_middle - 1\n",
    "        for i in range(layers_before_middle, 0, -1):\n",
    "            hidden_layer_sizes[index] = ceil(2*(self.input_layer_size + hidden_layer_sizes[i])/3)\n",
    "            index -= 1\n",
    "        \n",
    "        index = layers_before_middle + 1\n",
    "        for i in range(layers_before_middle, number_of_layers - 1, 1):\n",
    "            hidden_layer_sizes[index] = ceil(2*(hidden_layer_sizes[i] + self.output_layer_size)/3)\n",
    "            index += 1\n",
    "        return hidden_layer_sizes\n",
    "\n",
    "    def __create_network(self, middle_layer_size, number_of_layers, epochs, alpha):\n",
    "        network = NeuralNetwork()\n",
    "        hidden_layer_sizes = self.__interpolate_hidden_layer_sizes(middle_layer_size, number_of_layers)\n",
    "        hidden_layer_sizes.insert(0, self.input_layer_size)\n",
    "        hidden_layer_sizes.append(self.output_layer_size)\n",
    "        for i in range(len(hidden_layer_sizes) - 1):\n",
    "            network.add(FCLayer(hidden_layer_sizes[i], hidden_layer_sizes[i + 1]))\n",
    "            if i < len(hidden_layer_sizes) - 1:\n",
    "                network.add(ActivationLayer(self.activation_function, self.activation_function_derivative))\n",
    "        if self.type == \"Classification\":\n",
    "            network.add(ActivationLayer(sigmoid, sigmoid_derivative))\n",
    "            network.use(mse, mse_derivative)  #TODO: change loss function\n",
    "        else: \n",
    "            network.add(ActivationLayer(identity, identity_derivative))\n",
    "            network.use(mse, mse_derivative)\n",
    "        network.fit(self.X_train, self.y_train, epochs=epochs, learning_rate=alpha)\n",
    "        return network\n",
    "            \n",
    "    def __network_generator(self, config, reporter):\n",
    "        network = self.__create_network(config[\"middle_layer_size\"], config[\"number_of_layers\"], config[\"epochs\"], config[\"alpha\"])\n",
    "        y_pred = np.array(network.predict(self.X_test))\n",
    "        if self.type == \"Classification\":\n",
    "            reporter(config, mean_accuracy = accuracy_score(self.y_test, y_pred.round()), network=network)\n",
    "        else:\n",
    "            reporter(config, mean_loss = mean_squared_error(self.y_test, y_pred))\n",
    "        \n",
    "    def __train_network(self):\n",
    "        scheduler = AsyncHyperBandScheduler()\n",
    "        if self.type == \"Classification\":\n",
    "            res = tune.run(\n",
    "            self.__network_generator,\n",
    "            name=\"my_exp\",\n",
    "            metric=\"mean_accuracy\",\n",
    "            stop={\"mean_accuracy\": 0.9},\n",
    "            mode=\"max\",\n",
    "            scheduler=scheduler,\n",
    "            config={\n",
    "                \"number_of_layers\": tune.grid_search([6, 7, 8, 9]),\n",
    "                \"middle_layer_size\": tune.grid_search([8, 16, 32]),#, 64, 128]),\n",
    "                \"epochs\": tune.grid_search([500, 1000]),#, 2000, 3000, 4000, 5000]),\n",
    "                \"alpha\": tune.grid_search([0.001, 0.01, 0.1]),#, 1]),\n",
    "            },\n",
    "            )\n",
    "            results = {k: v for k, v in sorted(res.results.items(), key=lambda item: (item[1][\"mean_accuracy\"], -item[1][\"time_this_iter_s\"]), reverse=True)}\n",
    "                \n",
    "        else:\n",
    "            res = tune.run(\n",
    "            self.__network_generator,\n",
    "            name=\"my_exp\",\n",
    "            metric=\"mean_loss\",\n",
    "            stop={\"mean_loss\": 0.2},\n",
    "            mode=\"min\",\n",
    "            scheduler=scheduler,\n",
    "            config={\n",
    "                \"number_of_layers\": tune.grid_search([3, 4, 5]),\n",
    "                \"middle_layer_size\": tune.grid_search([8, 16, 32, 64, 128]),\n",
    "                \"epochs\": tune.grid_search([500, 1000, 2000, 3000, 4000, 5000]),\n",
    "                \"alpha\": tune.grid_search([0.001, 0.01, 0.1, 1]),\n",
    "            },\n",
    "            )\n",
    "            results = {k: v for k, v in sorted(res.results.items(), key=lambda item: (item[1][\"mean_loss\"], item[1][\"time_this_iter_s\"]))}\n",
    "        return list(results.values())[0]\n",
    "    \n",
    "    def get_best_network(self):\n",
    "        best_result = self.__train_network()\n",
    "        print(best_result)\n",
    "        accuracy = best_result[\"mean_accuracy\"] if self.type == \"Classification\" else best_result[\"mean_loss\"]\n",
    "        config = best_result[\"config\"]\n",
    "        network = best_result[\"network\"]\n",
    "        #network = self.__create_network(config[\"middle_layer_size\"], config[\"number_of_layers\"], config[\"epochs\"], config[\"alpha\"])\n",
    "        return network, accuracy, config\n",
    "    \n",
    "    def import_model(self, filepath):\n",
    "        with open(filepath, 'rb') as f:\n",
    "            res = pickle.load(f, encoding='bytes')\n",
    "        if not isinstance(res, NeuralNetwork):\n",
    "            raise TypeError('File does not exist or is corrupted')\n",
    "        return res\n",
    "\n",
    "    def export_model(self, filepath, network):\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(network, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_generator = NetworkGenerator(X_train, y_train, X_test, y_test, \"Classification\")\n",
    "network = network_generator.get_best_network()\n",
    "#sizes = network_generator.interpolate_hidden_layer_sizes(128, 5)\n",
    "#print(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array(network[0].predict(X_test))\n",
    "print(accuracy_score(y_test, y_pred.round()), network[1], network[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "training_data = pd.read_csv('suggestions.csv')\n",
    "\n",
    "for col in ['Age', 'Fare']:\n",
    "    normalized = normalize([np.asarray(training_data[col])])\n",
    "    training_data.drop(col, axis=1, inplace=True)\n",
    "    training_data[col] = normalized[0]\n",
    "        \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    training_data.loc[:, training_data.columns != 'Survived'], training_data[\"Survived\"], test_size=0.20)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_generator = NetworkGenerator(X_train, y_train, X_test, y_test, \"Classification\")\n",
    "network = network_generator.get_best_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array(network[0].predict(X_test))\n",
    "print(accuracy_score(y_test, y_pred.round()), network[1], network[2])\n",
    "print(classification_report(y_test, y_pred.round(), zero_division=1))\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b9bf3d753fefe854781e52229fcc2b6d37fd5cec0eed166290fc2ac2cd3389d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
