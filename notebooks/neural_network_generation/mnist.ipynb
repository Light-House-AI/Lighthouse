{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from neural_network import NeuralNetwork\n",
    "from fc_layer import FCLayer\n",
    "from activation_layer import ActivationLayer\n",
    "from activation_functions import tanh, tanh_derivative\n",
    "from loss_functions import mse, mse_derivative\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# load MNIST from server\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# training data : 60000 samples\n",
    "# reshape and normalize input data\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, 28*28)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255\n",
    "# encode output which is a number in range [0,9] into a vector of size 10\n",
    "# e.g. number 3 will become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "\n",
    "# same for test data : 10000 samples\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, 28*28)\n",
    "x_test = x_test.astype('float32')\n",
    "x_test /= 255\n",
    "y_test = np_utils.to_categorical(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network\n",
    "net = NeuralNetwork()\n",
    "net.add(FCLayer(28*28, 100))                # input_shape=(1, 28*28)    ;   output_shape=(1, 100)\n",
    "net.add(ActivationLayer(tanh, tanh_derivative))\n",
    "net.add(FCLayer(100, 50))                   # input_shape=(1, 100)      ;   output_shape=(1, 50)\n",
    "net.add(ActivationLayer(tanh, tanh_derivative))\n",
    "net.add(FCLayer(50, 10))                    # input_shape=(1, 50)       ;   output_shape=(1, 10)\n",
    "net.add(ActivationLayer(tanh, tanh_derivative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/35   error=0.247457\n",
      "epoch 2/35   error=0.096690\n",
      "epoch 3/35   error=0.076409\n",
      "epoch 4/35   error=0.064422\n",
      "epoch 5/35   error=0.055921\n",
      "epoch 6/35   error=0.049695\n",
      "epoch 7/35   error=0.044526\n",
      "epoch 8/35   error=0.040069\n",
      "epoch 9/35   error=0.036199\n",
      "epoch 10/35   error=0.032691\n",
      "epoch 11/35   error=0.029799\n",
      "epoch 12/35   error=0.027404\n",
      "epoch 13/35   error=0.025275\n",
      "epoch 14/35   error=0.023511\n",
      "epoch 15/35   error=0.021825\n",
      "epoch 16/35   error=0.020281\n",
      "epoch 17/35   error=0.019148\n",
      "epoch 18/35   error=0.018074\n",
      "epoch 19/35   error=0.017117\n",
      "epoch 20/35   error=0.016306\n",
      "epoch 21/35   error=0.015677\n",
      "epoch 22/35   error=0.014880\n",
      "epoch 23/35   error=0.014311\n",
      "epoch 24/35   error=0.013729\n",
      "epoch 25/35   error=0.013237\n",
      "epoch 26/35   error=0.012722\n",
      "epoch 27/35   error=0.012352\n",
      "epoch 28/35   error=0.011944\n",
      "epoch 29/35   error=0.011522\n",
      "epoch 30/35   error=0.011104\n",
      "epoch 31/35   error=0.010772\n",
      "epoch 32/35   error=0.010421\n",
      "epoch 33/35   error=0.010162\n",
      "epoch 34/35   error=0.009806\n",
      "epoch 35/35   error=0.009531\n",
      "\n",
      "\n",
      "predicted values : \n",
      "[array([[ 0.00462155, -0.0060067 , -0.10531852,  0.02307572, -0.36372225,\n",
      "         0.12640359,  0.02821931,  0.95671276,  0.06384859,  0.02315754]]), array([[ 9.47356551e-01,  6.60765012e-04,  3.50087614e-01,\n",
      "         7.20257038e-03, -8.97883699e-02, -6.48398147e-01,\n",
      "         2.31370353e-01, -8.56913262e-02,  1.90068803e-01,\n",
      "         8.96969779e-02]]), array([[-0.00389793,  0.98755314, -0.1760081 , -0.01453164, -0.31751043,\n",
      "         0.20390653, -0.10402344, -0.11810529,  0.05963719, -0.02310056]])]\n",
      "true values : \n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# train on 1000 samples\n",
    "# as we didn't implemented mini-batch GD, training will be pretty slow if we update at each iteration on 60000 samples...\n",
    "net.use(mse, mse_derivative)\n",
    "net.fit(x_train[0:1000], y_train[0:1000], epochs=35, learning_rate=0.1)\n",
    "\n",
    "# test on 3 samples\n",
    "out = net.predict(x_test[0:3])\n",
    "print(\"\\n\")\n",
    "print(\"predicted values : \")\n",
    "print(out, end=\"\\n\")\n",
    "print(\"true values : \")\n",
    "print(y_test[0:3])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "009c6260f8c949f949e8d5c80e91b7d11da42068722b763148e110eb4d6efd9c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
