{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import sys, os\n",
    "#sys.path.append(os.path.abspath('../../src/neural_network'))\n",
    "#sys.path.append('../../src/neural_network')\n",
    "from neural_network import NeuralNetwork\n",
    "from fc_layer import FCLayer\n",
    "from activation_layer import ActivationLayer\n",
    "from activation_functions import tanh, tanh_derivative, relu, relu_derivative\n",
    "from loss_functions import mse, mse_derivative\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "import sklearn\n",
    "from sklearn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10) (442,)\n",
      "[151. 317. 141. 131. 127. 263.  66. 257. 274.  37. 292. 166. 167. 173.\n",
      " 104. 272. 134. 108.  81.  88. 109. 264. 150. 237.  93. 310.  75.  65.\n",
      "  68.  64. 259. 104. 214. 232. 259. 103. 136. 152. 277. 177. 124. 219.\n",
      " 265. 107. 134. 236.  52.  47. 178.  68. 125. 122.  39. 163. 170.  96.\n",
      " 122.  90. 143. 235.  45.  59. 111. 281. 182.  67. 198.  60.  70. 243.\n",
      " 214. 101.  52. 268.  83. 102.  96. 116.  84. 168. 220. 171. 242.  77.\n",
      " 249.  72. 275. 180. 143.]\n",
      "(353, 1, 10) (89, 1, 10) (353, 1, 1) (89,)\n"
     ]
    }
   ],
   "source": [
    "X, Y = datasets.load_diabetes(return_X_y=True)\n",
    "print(X.shape, Y.shape)\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X,Y, test_size=0.2)\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "print(y_test)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[151. 317. 141. 131. 127. 263.  66. 257. 274.  37. 292. 166. 167. 173.\n",
      " 104. 272. 134. 108.  81.  88. 109. 264. 150. 237.  93. 310.  75.  65.\n",
      "  68.  64. 259. 104. 214. 232. 259. 103. 136. 152. 277. 177. 124. 219.\n",
      " 265. 107. 134. 236.  52.  47. 178.  68. 125. 122.  39. 163. 170.  96.\n",
      " 122.  90. 143. 235.  45.  59. 111. 281. 182.  67. 198.  60.  70. 243.\n",
      " 214. 101.  52. 268.  83. 102.  96. 116.  84. 168. 220. 171. 242.  77.\n",
      " 249.  72. 275. 180. 143.] (89,)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] (89,)\n",
      "29440.977528089887\n"
     ]
    }
   ],
   "source": [
    "network = NeuralNetwork()\n",
    "network.add(FCLayer(X_train.shape[2], 5))\n",
    "network.add(ActivationLayer(relu, relu_derivative))\n",
    "network.add(FCLayer(5, 2))\n",
    "network.add(ActivationLayer(relu, relu_derivative))\n",
    "network.add(FCLayer(2, 1))\n",
    "network.add(ActivationLayer(relu, relu_derivative))\n",
    "network.use(mse, mse_derivative)\n",
    "network.fit(X_train, y_train, epochs=100, learning_rate=0.001)\n",
    "y_pred = np.array(network.predict(X_test))\n",
    "y_pred = y_pred.reshape(y_pred.shape[0],)\n",
    "print(y_test, y_test.shape)\n",
    "print(y_pred, y_pred.shape)\n",
    "print(sklearn.metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func(config, reporter):  # add the reporter parameter\n",
    "  # report the configuration\n",
    "  network = NeuralNetwork()\n",
    "  network.add(FCLayer(config['input_dim'], config['hidden_dim']))\n",
    "  network.add(ActivationLayer(relu, relu_derivative))\n",
    "  network.add(FCLayer(config['hidden_dim'], 2))\n",
    "  network.add(ActivationLayer(relu, relu_derivative))\n",
    "  network.add(FCLayer(2, config['output_dim']))\n",
    "  network.add(ActivationLayer(relu, relu_derivative))\n",
    "  network.use(mse, mse_derivative)\n",
    "  network.fit(X_train, y_train, epochs=100, learning_rate=config[\"alpha\"])\n",
    "  y_pred = np.array(network.predict(X_test))\n",
    "  y_pred = y_pred.reshape(y_pred.shape[0],)\n",
    "  reporter(config, mean_loss = sklearn.metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 06:29:07,431\tWARNING bayesopt.py:397 -- BayesOpt does not support specific sampling methods. The Uniform sampler will be dropped.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 6.5/7.9 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: None<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/0.81 GiB heap, 0.0/0.4 GiB objects<br>Result logdir: C:\\Users\\Mohamed Ghallab\\ray_results\\my_exp<br>Number of trials: 1/10 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name      </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  alpha</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>my_func_6c6a401d</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">3.74546</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 06:29:11,631\tWARNING tune.py:518 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 6.7/7.9 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: None<br>Resources requested: 2.0/8 CPUs, 0/0 GPUs, 0.0/0.81 GiB heap, 0.0/0.4 GiB objects<br>Result logdir: C:\\Users\\Mohamed Ghallab\\ray_results\\my_exp<br>Number of trials: 3/10 (1 PENDING, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name      </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  alpha</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>my_func_6c6a401d</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">3.74546</td></tr>\n",
       "<tr><td>my_func_6c7f2ec1</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">9.50715</td></tr>\n",
       "<tr><td>my_func_6c916212</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">7.31997</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 06:29:11,880\tERROR tune.py:557 -- Trials did not complete: [my_func_6c6a401d, my_func_6c7f2ec1, my_func_6c916212]\n",
      "2022-04-14 06:29:11,881\tINFO tune.py:561 -- Total run time: 4.46 seconds (4.20 seconds for the tuning loop).\n",
      "2022-04-14 06:29:11,882\tWARNING tune.py:565 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n",
      "2022-04-14 06:29:11,897\tWARNING experiment_analysis.py:644 -- Could not find best trial. Did you pass the correct `metric` parameter?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found were:  None\n"
     ]
    }
   ],
   "source": [
    "algo = BayesOptSearch(utility_kwargs={\"kind\": \"ucb\", \"kappa\": 2.5, \"xi\": 0.0})\n",
    "algo = ConcurrencyLimiter(algo, max_concurrent=4)\n",
    "scheduler = AsyncHyperBandScheduler()\n",
    "analysis = tune.run(\n",
    "        my_func,\n",
    "        name=\"my_exp\",\n",
    "        metric=\"mean_loss\",\n",
    "        mode=\"min\",\n",
    "        search_alg=algo,\n",
    "        scheduler=scheduler,\n",
    "        num_samples=10,\n",
    "        config={\n",
    "            \"steps\": 100,\n",
    "            \"input_dim\": X_train.shape[2],\n",
    "            \"hidden_dim\": 20,\n",
    "            \n",
    "            \"output_dim\": 1,\n",
    "            \"alpha\": tune.uniform(0.0001, 10),\n",
    "        },\n",
    "    )\n",
    "\n",
    "print(\"Best hyperparameters found were: \", analysis.best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "009c6260f8c949f949e8d5c80e91b7d11da42068722b763148e110eb4d6efd9c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
